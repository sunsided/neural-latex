\documentclass[a4paper, 11pt, ngerman, english]{article}

\usepackage{eurosym}
\usepackage{geometry}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage[figurewithin=section, 
		   font=small, 
		   labelfont=bf]
		   {caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{inconsolata}
\usepackage{lmodern}
\usepackage[ngerman,english]{babel}

\usepackage{makeidx}

\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{cancel}

\usepackage{color}
\usepackage{pgfplots}
\pgfplotsset{compat=1.11}

\usepackage{varioref}
\usepackage[pdftex]{hyperref}
\usepackage{cleveref}
\usepackage{siunitx}
\usepackage{todonotes}

\usepackage[babel]{csquotes}
\usepackage[backend=biber,style=authoryear]{biblatex}

\usepackage{listings}

% color definitions for listings
\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstset{literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\EUR}}1 {£}{{\pounds}}1
}

\newcommand{\listingpartial}{\mbox{$\partial$}}

%\lstdefinestyle{csharp}{
%	language=[Sharp]C,
%}

\lstdefinestyle{matlab}{
	language=matlab,
	showspaces=false,
	showtabs=false,
	breaklines=true,
	showstringspaces=false,
	breakatwhitespace=true,
	escapeinside={/*@}{@*/},
	backgroundcolor=\color{backcolour},  
	commentstyle=\color{greencomments},
	keywordstyle=\color{bluekeywords}\bfseries,
	stringstyle=\color{redstrings},
	basicstyle=\footnotesize\ttfamily,
	columns=fullflexible,
	tabsize=2,
	numbers=left,                    
    numbersep=5pt,
    numberblanklines=true,
    morekeywords={syms,pretty}
}

% cleveref bindings for other packages
\crefname{lstlisting}{listing}{listings}
\Crefname{lstlisting}{Listing}{Listings}

\bibliography{quellen.bib}

\geometry{a4paper,
		top=25mm, 
		left=40mm, 
		right=25mm, 
		bottom=30mm, 
		headsep=10mm, 
		footskip=12mm}

\graphicspath{ {./images/} }

\pagestyle{plain}
\pagenumbering{arabic}

% Generate the index and glossary
\makeindex

\newcommand{\name}[1]{\textsc{#1}}

\begin{titlepage}
	\title{\textbf{Neural Networks} \\ equations on overdrive}
	\author{Markus Mayer}
	\date{\today}
\end{titlepage}

\begin{document}

%Term definitions
%\newglossaryentry{sse}{name=SSE, description={Sum of Squared Errors}}

\maketitle

\begin{abstract}
\textit{Artificial Neural Networks} are like regular neural networks, except they're artificially complicated.
\end{abstract}

\tableofcontents
\clearpage

\section{Feedforward estimation and error backpropagation}

\section{Connection weight optimization}

In order to determine the neural connection weights $\theta$, a multitude of local and global optimization algorithms can be used.
One of the most widely used (or at least discussed) algorithms \todo{yeah ... where?} is the \textit{gradient descent}, described in \cref{sec:gradientdescent}.
This algorithm and its variants observe the network's output error with regard to the connection weights during training (\cref{sec:costfunction}) and interpret it as a hilly landscape in parameter-space, where each valley represents parameters of (locally) lowest error and different variations of connection weights represent different hill formations.
The objective is then to efficiently "climb downwards" these hills until a valley is reached, ideally finding the lowest valley, associated with zero error (i.e. a perfect network output).
A visual example for such a landscape and the name-giving gradients can be found in \cref{fig:sselike_cost}.

\todo{conjugate gradients}
\todo{Kalman}
\todo{BFGS}

\subsection{The gradient descent algorithm}
\label{sec:gradientdescent}

Learning rate $\lambda$\label{gdlearningrate}.% The learning rate is cross-referenced when talking about the first derivative of the cost function

\section{The cost function and its gradient}
\label{sec:costfunction}

Training neural networks is an optimization problem where we want to find the network weights in such a way that the network's output closely resembles the expected output.
Specifically, we want to find the weight parameters that minimize the error between the network's output and the expected (training) output.
In order to perform optimisation we need to be able to define a \textit{cost} metric $J$ for the network's training, and in our case that cost will be said training error.

The error $e$ of the network given the parameters $\theta$ is simply the difference between the network's output $h_\theta(x)$ and the expected output $y$, such that
%
\begin{align}
e &= h_\theta(x) - y
\end{align}

\subsection{The sum of squared errors}

A widely used and relatively simple differentiable error metric is the \textit{sum of squared errors} (SSE) that sums the (squared) errors over all training examples $\{x^{(i)}, y^{(i)}\}$:
%
\begin{align}
E_{\mathrm{SSE}}(\theta) &= \sum_{i=1}^m \left( e^{(i)} \right)^2 \\
&= \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
\end{align}

In order to make the cost function $J$ independent of the number of training examples, we will define it as (one half) the average sum of squared errors:
%
\begin{align}
J(\theta) % &= \frac{1}{2} \left( \frac{1}{m} E_{\mathrm{SSE}}(\theta) \right) \\
          &= \frac{1}{2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
\end{align}

The idea of including the factor $\sfrac{1}{2}$ here is that scaling the cost metric does not change its qualitative meaning; it will do us a little favor when differentiating later, however.\footnote{Some sources (such as \cite{DBLP:books/daglib/0031855}) incorporate this factor into the learning rate of the Gradient Descent algorithm instead, which is covered in \cref{sec:gradientdescent} \vpageref{gdlearningrate}.}

The cost function $J(\theta)$ is not a function of the training pairs $\{x, y\}$ --- note that it is calculated over the complete training set ---, but rather a function of the network's parameter vector $\theta$.
In order to determine in which way changing any of these parameters will change the cost, we will inspect the partial derivative of $J(\theta)$ with respect to each individual component $\theta_j \in \theta$, which is called the gradient $\nabla J(\theta)$.

Since the first derivative of a function describes the slope of that function at the point of evaluation, it can be used to determine the direction of the nearest local maximum (or, by sign flip, minimum). This is visualized by 
\cref{fig:sselike_cost} \vpageref{fig:sselike_cost} using the function $f(a,b)=3a^2+b^2$.

Consequently, the vector of all partial derivatives of $J(\theta)$ with respect to $\theta$ describes the direction in parameter space in which the nearest local maximum (or minimum) of the cost function can be found.

\begin{figure}[H]
	\begin{subfigure}[b]{\textwidth}
	\centering
	\input{images/error_mesh.tikz}
	\caption[Surface plot of $f(a,b)=3a^2+b^2$.]{Surface plot of $f(a,b)=3a^2+b^2$. Shows the function values evaluated at different $a,b$. One global minimum exists at $\{0, 0\}$.}
	\label{fig:sselike_cost_mesh}
	\end{subfigure}

	\begin{subfigure}[b]{\textwidth}
	\centering
	\input{images/error_quiver.tikz}
	\caption[Quiver plot of $f(a,b)=3a^2+b^2$.]{Gradient vector field for $f(a,b)=3a^2+b^2$. Length and direction of the vectors represent magnitude and sign of the partial derivative, which can be interpreted as the "uphill" direction of that axis. The dotted vector represents the resulting gradient. Arrow lengths are scaled for display purposes.}
	\label{fig:sselike_cost_quiver}
	\end{subfigure}
	
	\caption{Visualization of $f(a,b)=3a^2+b^2$ and its partial derivatives.}
	\label{fig:sselike_cost}
\end{figure}

\subsubsection{First derivative of the squared error function}

The gradient of the squared error cost function is given as:
%
\begin{align}
\nabla J(\theta) &= \frac{\partial}{\partial \theta} J(\theta) %\\
= \frac{\partial}{\partial \theta} \frac{1}{2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)}\right)^2 \label{eq:nablaJ}
\end{align}

The sum rule%
\footnote{Given $f(x) = a(x) + b(x)$, then $f'(x) = a'(x) + b'(x)$.}%
of differentiation gives that the derivative of a sum is the sum of the derivatives, such that \cref{eq:nablaJ} becomes
%
\begin{align}
\nabla J(\theta) &= \sum_{i=1}^m \enskip \frac{\partial}{\partial \theta} \frac{1}{2m} \left( h_\theta(x^{(i)}) - y^{(i)}\right)^2 \label{eq:nablaJsorted}
\end{align}
%
which leaves us with with the problem of finding $\frac{\partial}{\partial \theta} \frac{1}{2m} \left( h_\theta(x^{(i)}) - y^{(i)}\right)^2$.
By application of the chain rule%
\footnote{Given $f(x) = a(b(x))$, then $f'(x) = a'(b(x)) \cdot b'(x)$.}%
, we find that
%
\begin{align}
\frac{\partial}{\partial \theta} \frac{1}{2m} \left( h_\theta(x^{(i)}) - y^{(i)}\right)^2 &= 
%
\frac{1}{\cancel{2}m} 
%\enskip \cdot \enskip 
\, \cdot \, 
\underbrace{\cancel{2} \left( h_\theta(x^{(i)}) - y^{(i)}\right)}_{\text{outer derivative}}
%
%\enskip \cdot \enskip
\, \cdot \, 
\underbrace{
\frac{\partial}{\partial \theta}
\left( h_\theta(x^{(i)}) - y^{(i)}\right)}_{\text{inner derivative}}
 \label{eq:partialJ}
\end{align}
%
where the inner derivative is
%
\begin{align}
\frac{\partial}{\partial \theta}
\left( h_\theta(x^{(i)}) - y^{(i)}\right)
&=
\frac{\partial}{\partial \theta}
h_\theta(x^{(i)}) \label{eq:innerJ}
\end{align}

Using \cref{eq:partialJ,eq:innerJ} in \cref{eq:nablaJsorted} and moving the fraction out of the sum gives us
%
\begin{align}
\nabla J(\theta) &=
\frac{1}{m} \sum_{i=1}^m 
\, 
\left( h_\theta(x^{(i)}) - y^{(i)}\right)
\, \cdot \, 
\frac{\partial}{\partial \theta} h_\theta(x^{(i)}) \label{eq:sse1stDerivative}
\end{align}

\Cref{lst:matlab-1st-derivative} shows a Matlab script to determine the first derivative of the squared error function.
Because $- (y - h_\theta(x)) = (-y + h_\theta(x)) = (h_\theta(x) - y)$, the output is equivalent to \cref{eq:partialJ}.

\begin{lstlisting}[label=lst:matlab-1st-derivative,caption=First derivative of the squared error function in Matlab, style=matlab]
>> syms h(t,x) y m
>> diff(1/(2*m)*(h(t,x)-y)^2, t)

ans =
 
-(diff(h(t, x), t)*(y - h(t, x)))/m
\end{lstlisting}

\subsubsection{Second derivative of the squared error function}

The second derivative of the squared-error function can be obtained by differentiating \cref{eq:sse1stDerivative}, such that
%
\begin{align}
\nabla\left(\nabla J(\theta)\right) = \frac{\partial^2}{\partial \theta^2} J(\theta) &=
\frac{\partial}{\partial \theta} \left[
\frac{1}{m} \sum_{i=1}^m 
\, 
\left( h_\theta(x^{(i)}) - y^{(i)}\right)
\, \cdot \, 
\frac{\partial}{\partial \theta} h_\theta(x^{(i)})
\right] 
\\
&=
\frac{1}{m} \sum_{i=1}^m 
\, 
\frac{\partial}{\partial \theta}
\left[
\left( h_\theta(x^{(i)}) - y^{(i)}\right)
\, \cdot \, 
\frac{\partial}{\partial \theta} h_\theta(x^{(i)})
\right]
\end{align}

By application of the product rule%
\footnote{Given $f(x) = a(x)\cdot b(x)$, then $f'(x) = a'(x)\cdot b(x) + a(x)\cdot b'(x)$.}%
we obtain
%
\begin{align}
\begin{split}
\frac{\partial}{\partial \theta}
\left[
\left( h_\theta(x^{(i)}) - y^{(i)} \right)
\, \cdot \, 
\frac{\partial}{\partial \theta} h_\theta(x^{(i)})
\right]
&=
\frac{\partial}{\partial \theta}
\left( h_\theta(x^{(i)}) - y^{(i)}\right)
\, \cdot \, 
\frac{\partial}{\partial \theta}
h_\theta(x^{(i)}) \\
&+
\left( h_\theta(x^{(i)}) - y^{(i)}\right)
\, \cdot \, 
\frac{\partial^2}{\partial \theta^2}
h_\theta(x^{(i)})
\end{split} \label{eq:sse2ndDerivativeSteps}
\end{align}

In the first part, the sum rule is applied again, giving $\frac{\partial}{\partial \theta}
\left( h_\theta(x^{(i)}) - y^{(i)}\right) = \frac{\partial}{\partial \theta}\,
h_\theta(x^{(i)}) - \frac{\partial}{\partial \theta}\, y^{(i)}$. Since $y^{(i)}$ is independent of $\theta$, it follows that $\frac{\partial}{\partial \theta}\, y^{(i)} = 0$, such that
%
\begin{align}
\frac{\partial}{\partial \theta}
\left( h_\theta(x^{(i)}) - y^{(i)}\right)
\, \cdot \, 
\frac{\partial}{\partial \theta}
h_\theta(x^{(i)})
&=
\left( 
\frac{\partial}{\partial \theta}
h_\theta(x^{(i)})\right)^2
\end{align}
%
which gives the final result in \cref{eq:sse2ndDerivative}:
%
\begin{align}
\nabla\left(\nabla J(\theta)\right) &= 
\frac{1}{m} \sum_{i=1}^m 
\left[
\left(\frac{\partial}{\partial \theta} h_\theta(x^{(i)})\right)^2 +\left( h_\theta(x^{(i)}) - y^{(i)} \right) \cdot \frac{\partial^2}{\partial \theta^2} h_\theta(x^{(i)}) 
\right] \label{eq:sse2ndDerivative}
\end{align}
%
meaning that the second derivative of the squared error cost function is governed by the first and second derivatives of the hypothesis $h_\theta(x)$ (e.g. the network).

Last but not least, the Matlab script in \Cref{lst:matlab-2nd-derivative} prints out the same result as \cref{eq:sse2ndDerivative}, apart from a factored-out sign (similar to \cref{lst:matlab-1st-derivative}).

\begin{lstlisting}[label=lst:matlab-2nd-derivative,caption=Second derivative of the squared error function in Matlab, style=matlab]
>> syms h(t,x) y m
>> diff(1/(2*m)*(h(t,x)-y)^2, t, 2)

ans =
 
diff(h(t, x), t)^2/m - ((y - h(t, x))*diff(h(t, x), t, t))/m
\end{lstlisting}


\subsection{Logistic regression cost}

For binary classification problems, i.e. when $y_i \in \{0, 1\}$, the logistic regression cost function
%
\begin{align}
J(\theta) &= \frac{1}{m} \sum_{i=1}^m e^{(i)}
\quad \text{with} \quad
e^{(i)} = 
\begin{cases}
- \log \left( h_\theta(x^{(i)}) \right) \quad &\text{if} \quad y^{(i)} = 1\\
- \log \left( 1 - h_\theta(x^{(i)}) \right) \quad &\text{if} \quad y^{(i)} = 0
\end{cases}
\label{eq:logRegCostBranch}
\end{align}
%
is suggested to be used instead of the squared error (\cite{NgCourseraML2015}).
\Cref{fig:logreg_cost} \vpageref{fig:logreg_cost} shows the resulting curves for $y=0$ and $y=1$ respectively.
Since the possible values for $y^{(i)}$ are are discrete and binary, \cref{eq:logRegCostBranch} can be written as
%
\begin{align}
J(\theta) &= -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log \left( h_\theta(x^{(i)}) \right)  + \left(1 - y^{(i)}\right) \log \left( 1 - h_\theta(x^{(i)}) \right) \right]
\end{align}
%
for branchless evaluation.

\begin{figure}[H]
	\centering
	\input{images/logreg_cost.tikz}
	\caption[Logistic regression cost function for $y=0$ and $y=1$.]{Logistic regression cost function $J(\theta)$. For $y=0$ and $h_\theta(x) \gg 0$ the hypothesis will be penalized with a high cost, likewise for $y=1$ and $h_\theta(x) \ll 1$. For $h_\theta(x) = y$ the cost will be zero.}
	\label{fig:logreg_cost}
\end{figure}



% print the list of figures
\addcontentsline{toc}{section}{List of Figures}
\listoffigures

\printbibliography

\end{document}